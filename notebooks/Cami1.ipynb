{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future self, move notebook and artnames.txt to outermost directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py7zr as pz\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from src.libcode import list_to_txt, txt_to_list\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_unzip(link):\n",
    "    resp = requests.get(i, stream=True)\n",
    "    z = pz.SevenZipFile(io.BytesIO(resp.content))\n",
    "    z.extractall(\"src/data\")\n",
    "#     with pz.SevenZipFile('src/data/' + fname, mode='r') as z:\n",
    "#         z.extractall(\"src/data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(fname=\"artnames.txt\"):\n",
    "    resp = requests.get(\"https://dumps.wikimedia.org/enwiki/20210101/\")\n",
    "    dump_soup = BeautifulSoup(resp.text)\n",
    "    dumps_base = \"https://dumps.wikimedia.org\"\n",
    "    link_list = []\n",
    "    for i in dump_soup.find_all(\"a\"):\n",
    "        if \"pages-meta-history\" in i.text and \".7z\" in i.text:\n",
    "            link_list.append(dumps_base + i[\"href\"])\n",
    "    list_to_txt(fname, link_list)\n",
    "    \n",
    "def retrieve_links(fname = \"artnames.txt\"):\n",
    "    return txt_to_list(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list = retrieve_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://dumps.wikimedia.org/enwiki/20210101/enwiki-20210101-pages-meta-history1.xml-p1p873.7z',\n",
       " 'https://dumps.wikimedia.org/enwiki/20210101/enwiki-20210101-pages-meta-history1.xml-p874p1816.7z',\n",
       " 'https://dumps.wikimedia.org/enwiki/20210101/enwiki-20210101-pages-meta-history1.xml-p1817p2753.7z',\n",
       " 'https://dumps.wikimedia.org/enwiki/20210101/enwiki-20210101-pages-meta-history1.xml-p2754p3454.7z',\n",
       " 'https://dumps.wikimedia.org/enwiki/20210101/enwiki-20210101-pages-meta-history1.xml-p3455p4320.7z']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "689"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6f667f65653b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdownload_unzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-5c5556182f2b>\u001b[0m in \u001b[0;36mdownload_unzip\u001b[0;34m(link)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdownload_unzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSevenZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"src/data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     with pz.SevenZipFile('src/data/' + fname, mode='r') as z:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "download_unzip(link_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../',\n",
       " '20201020/',\n",
       " '20201101/',\n",
       " '20201120/',\n",
       " '20201201/',\n",
       " '20201220/',\n",
       " '20210101/',\n",
       " '20210120/',\n",
       " 'latest/']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = 'https://dumps.wikimedia.org/enwiki/'\n",
    "index = requests.get(base_url).text\n",
    "soup_index = BeautifulSoup(index, 'html.parser')\n",
    "# Find the links on the page\n",
    "dumps = [a['href'] for a in soup_index.find_all('a') if \n",
    "         a.has_attr('href')]\n",
    "dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"latest\" not in dumps[-1]:\n",
    "    print(\"Couldn't find the latest dump\")\n",
    "    print(dumps[-1])\n",
    "    print(dumps[-2])\n",
    "    exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<li class=\"file\"><a href=\"/enwiki/20201020/enwiki-20201020-pages-articles-multistream.xml.bz2\">enwiki-20201020-pages-articles-multistream.xml.bz2</a> 17.6 GB</li>,\n",
       " <li class=\"file\"><a href=\"/enwiki/20201020/enwiki-20201020-pages-articles-multistream-index.txt.bz2\">enwiki-20201020-pages-articles-multistream-index.txt.bz2</a> 216.3 MB</li>,\n",
       " <li class=\"file\"><a href=\"/enwiki/20201020/enwiki-20201020-pages-articles-multistream1.xml-p1p41242.bz2\">enwiki-20201020-pages-articles-multistream1.xml-p1p41242.bz2</a> 232.1 MB</li>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump_url = base_url + '20201020/'\n",
    "# Retrieve the html\n",
    "dump_html = requests.get(dump_url).text\n",
    "# Convert to a soup\n",
    "soup_dump = BeautifulSoup(dump_html, 'html.parser')\n",
    "# Find list elements with the class file\n",
    "soup_dump.find_all('li', {'class': 'file'})[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of files\n",
    "* pages-articles-multistream.xml.bz2 – Current revisions only, no talk or user pages; this is probably what you want, and is approximately 18 GB compressed (expands to over 78 GB when decompressed).\n",
    "* pages-meta-current.xml.bz2 – Current revisions only, all pages (including talk)\n",
    "* abstract.xml.gz – page abstracts\n",
    "* all-titles-in-ns0.gz – Article titles only (with redirects)\n",
    "* SQL files for the pages and links are also available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('enwiki-20201020-pages-meta-current.xml.bz2', ['32.1', 'GB'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for \"current\" files\n",
    "files = []\n",
    "for file in soup_dump.find_all(\"li\", {\"class\": \"file\"}):\n",
    "    text = file.text\n",
    "    #print(file.text)\n",
    "    # what are these?\n",
    "    if \"pages-meta-current\" in text:\n",
    "        files.append((text.split()[0], text.split()[1:]))\n",
    "files_to_download = [file[0] for file in files]\n",
    "print(len(files_to_download))\n",
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
