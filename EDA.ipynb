{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi as wp\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk import stem\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.etl.get_anames import retrieve_anames\n",
    "from src.libcode import txt_to_list, list_to_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enwp = wp.Wikipedia(\"en\")\n",
    "anames = retrieve_anames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_dir = \"src/data/init/partisan_phrases/\"\n",
    "pp_txts = os.listdir(pp_dir)\n",
    "score_dict = {}\n",
    "for i in pp_txts:\n",
    "    with open(pp_dir + i) as curtxt:\n",
    "        for line in curtxt.readlines()[1:]:\n",
    "            splt = line.split(\"|\")\n",
    "            score_dict[splt[0]] = float(splt[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_per_text = len(anames)//10\n",
    "# wikitxts_dir = \"src/data/temp/wiki_txts/\"\n",
    "\n",
    "# # If wiki texts folder does not exist make it\n",
    "# if not os.path.exists(wikitxts_dir):\n",
    "#     os.makedirs(wikitxts_dir)\n",
    "\n",
    "# txtlst = []\n",
    "# for ind, aname in enumerate(anames):\n",
    "#     # Get the page text\n",
    "# #     print(ind, aname)\n",
    "#     curpg = enwp.page(aname)\n",
    "#     curtit = curpg.title\n",
    "#     curtxt = curpg.text\n",
    "#     txtlst.append(curtit)\n",
    "#     txtlst.append(curtxt)\n",
    "#     # This ensures it saves into 10 txt files\n",
    "#     if (ind+1) % num_per_text == 0:\n",
    "#         print(ind+1)\n",
    "#         curtxt_name = \"art_pages\" + str((ind+1)//num_per_text) + \".txt\"\n",
    "#         list_to_txt(wikitxts_dir+curtxt_name, txtlst)\n",
    "#         txtlst = []\n",
    "#         time.sleep(10)\n",
    "        \n",
    "# # Save last set of articles\n",
    "# if len(txtlst) > 0:\n",
    "#     curtxt_name = \"art_pages10\" + \".txt\"\n",
    "#     list_to_txt(wikitxts_dir+curtxt_name, txtlst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(score_dict.items(), key=lambda item: item[1], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download(\"wordnet\")\n",
    "# nltk.download(\"stopwords\")\n",
    "\n",
    "stpwrds = stopwords.words(\"english\")\n",
    "porter = stem.PorterStemmer()\n",
    "\n",
    "def preproc_strn(strn):\n",
    "    # Lowercase, remove digits and doublespaces\n",
    "    curstr = strn.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    curstr = re.sub(r'[0-9]+', '', curstr)\n",
    "    curstr = re.sub(r'\\n', ' ', curstr)\n",
    "    curstr = re.sub(r'  +', ' ', curstr)\n",
    "    plst = []\n",
    "    for word in curstr.split():\n",
    "        # Check for stopwords\n",
    "        if word not in stpwrds:\n",
    "            # Porter stem the word\n",
    "            pword = porter.stem(word)\n",
    "            plst.append(pword)\n",
    "    numwords = len(plst)\n",
    "    curstr = ' '.join(plst)\n",
    "    return (curstr, numwords)\n",
    "\n",
    "def string_score(strn, score_dict):\n",
    "    # Pre-process, return the processed string and the number of words\n",
    "    curstr, numwords = preproc_strn(strn)\n",
    "\n",
    "    # Absolute bias sum\n",
    "    absscore = 0\n",
    "    # Bias sum\n",
    "    sumscore = 0\n",
    "    # Total number of occurences of phrases from G&S\n",
    "    totphrs = 0\n",
    "    \n",
    "    # Dictionary of top 10 phrase counts\n",
    "    counts_dict = {}\n",
    "    \n",
    "    for key, value in score_dict.items():\n",
    "        \n",
    "        numoccurs = curstr.count(key)\n",
    "        totphrs += numoccurs\n",
    "        counts_dict[key] = (numoccurs, value)\n",
    "        curscore = numoccurs*value\n",
    "        absscore += abs(curscore)\n",
    "        sumscore += curscore\n",
    "\n",
    "    counts_list = sorted(counts_dict.items(), key=lambda item: item[1], reverse=True)[:10]\n",
    "    return [absscore, sumscore, numwords, counts_list, totphrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wikitxts_dir = \"src/data/temp/wiki_txts/\"\n",
    "wiki_txts = [\"art_pages\" + str(i) + \".txt\" for i in range(1,11)]\n",
    "namestat_dict = {}\n",
    "\n",
    "cnt = 0\n",
    "for txt in wiki_txts:\n",
    "    print(cnt)\n",
    "    txtlst = txt_to_list(wikitxts_dir + txt)\n",
    "    for item in txtlst:\n",
    "        if cnt % 2 == 0:\n",
    "            aname = item\n",
    "        else:\n",
    "            curres = string_score(item,score_dict)\n",
    "            namestat_dict[aname] = curres\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namestat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, stat in namestat_dict.items():\n",
    "    dispcnt = 1\n",
    "    print(name + \":\")\n",
    "    procname = preproc_strn(name)[0]\n",
    "    is_intitle = False\n",
    "    for phr, freq in stat[3]:\n",
    "        if phr in procname:\n",
    "            is_intitle = True\n",
    "        print(str(dispcnt) + \".    \" + phr + \" - \" + str(freq))\n",
    "        dispcnt += 1\n",
    "        \n",
    "    namestat_dict[name].append(is_intitle)\n",
    "            \n",
    "    print(\"__________________________\")\n",
    "    print()\n",
    "    \n",
    "# I still have to figure how to work with the frequencies the best. So far I only have a boolean for \"title in most frequent 10 phrases\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "namestat_df = pd.DataFrame.from_dict(namestat_dict,orient=\"index\",columns=[\"absscore\",\"sumscore\",\"numwords\",\"counts_list\",\"totphrs\",\"is_intitle\"]).reset_index()\n",
    "\n",
    "def make_columns(df):\n",
    "    df[\"abs_by_num\"] = df[\"absscore\"] / df[\"numwords\"]\n",
    "    df[\"sum_by_num\"] = df[\"sumscore\"] / df[\"numwords\"]\n",
    "    df_reset = df.reset_index().rename({\"level_0\":\"popularity\"}, axis=1)\n",
    "    abs_rank = df_reset.sort_values(by=\"absscore\", ascending=False).reset_index()[\"level_0\"]\n",
    "    sum_rank = df_reset.sort_values(by=\"sumscore\", ascending=False).reset_index()[\"level_0\"]\n",
    "    abn_rank = df_reset.sort_values(by=\"abs_by_num\", ascending=False).reset_index()[\"level_0\"]\n",
    "    sbn_rank = df_reset.sort_values(by=\"sum_by_num\", ascending=False).reset_index()[\"level_0\"]\n",
    "    df[\"abs_rank_diff\"] = abs_rank - abn_rank\n",
    "    df[\"sum_rank_diff\"] = sum_rank - sbn_rank\n",
    "    df[\"abs_rank\"] = abs_rank\n",
    "    df[\"sum_rank\"] = sum_rank\n",
    "    df[\"abn_rank\"] = abn_rank\n",
    "    df[\"sbn_rank\"] = sbn_rank\n",
    "    return df\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "nsdf = make_columns(namestat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(nsdf[\"sumscore\"]).corr(nsdf[\"numwords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsdf[\"numwords\"].corr(nsdf[\"absscore\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsdf[\"is_intitle\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(score_dict, orient=\"index\", columns = [\"score\"])[\"score\"].hist(bins=1000)\n",
    "plt.xlim([-200, 200])\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Bias scores\")\n",
    "plt.savefig('bias_hist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsdf[[\"absscore\",\"numwords\"]].plot(kind=\"scatter\",x=\"numwords\",y=\"absscore\")\n",
    "plt.xlabel(\"Number of words\")\n",
    "plt.ylabel(\"Absolute score sum\")\n",
    "plt.savefig('absnum_scat.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsdf[\"sumscoreabs\"] = abs(nsdf[\"sumscore\"])\n",
    "nsdf[[\"sumscoreabs\",\"numwords\"]].plot(kind=\"scatter\",x=\"numwords\",y=\"sumscoreabs\")\n",
    "plt.xlabel(\"Number of words\")\n",
    "plt.ylabel(\"Score sum\")\n",
    "plt.savefig('sum_scat.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
